{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Model v3.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomYOON/Stock-project/blob/master/Basic_Model_v3_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0hF7M50RySa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCeNc93yecPR",
        "colab_type": "text"
      },
      "source": [
        "# **Mount your drive**\n",
        "본인의 구글 드라이브와 Colab을 연동합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIs8LcpQ2NkM",
        "colab_type": "code",
        "outputId": "7e1b97ad-f92a-4861-d48a-5708838a8c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpbeiVKHV0KG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing data from url\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/inuki81/stock-data/master/kospi200_10y_git.csv\"\n",
        "csv_df=pd.read_csv(url)\n",
        "\n",
        "#  0     1    2    3     4      5    6    7\n",
        "# year,month,day,close,volume,start,high,low\n",
        "\n",
        "#date,close,start,high,low,volume\n",
        "\n",
        "csv_np_raw = csv_df.to_numpy()\n",
        "row,col = np.shape(csv_np_raw)\n",
        "\n",
        "full_data_np = csv_np_raw[:,1:]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zRtaPvmzskC",
        "colab_type": "text"
      },
      "source": [
        "# **input data 변경**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILEnJGCGzijW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  0     1    2    3     4      5    6    7\n",
        "# year,month,day,close,volume,start,high,low\n",
        "###################################################\n",
        "input_value = [1,0,2,3,4]\n",
        "data_np = full_data_np[:,input_value]      #####여기있는 숫자 변경하면 input data 변경됨#####\n",
        "##################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VswRwOMSLPcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalize data\n",
        "\n",
        "#data_min = np.min(data_np[:,1])\n",
        "#data_scale = np.max(data_np[:,1]) - np.min(data_np[:,1])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_np)\n",
        "data_norm = scaler.transform(data_np)\n",
        "data_norm = np.insert(data_norm,0,np.arange(data_norm.shape[0]),axis=1)\n",
        "#data = torch.from_numpy(data_norm)\n",
        "#plt.plot(data_norm[:,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTKTvc8yn05A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MoreBasicLSTM(nn.Module):   \n",
        "  def __init__(self, input_size, hidden, drop, output_size):\n",
        "    super().__init__()\n",
        "    self.date = output_size\n",
        "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden)\n",
        "    self.drop = nn.Dropout(p = drop)\n",
        "    self.fclayers = nn.Sequential(\n",
        "        nn.Softsign(),\n",
        "        nn.Linear(in_features = hidden, out_features = output_size)\n",
        "    )\n",
        "    \n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    _,(hidden,_) = self.lstm(inputs)\n",
        "    output = self.drop(hidden.squeeze(0))\n",
        "    output = self.fclayers(output).squeeze(0)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXlAPYcri7LZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MoreBasicLSTM_timestamp_processing(nn.Module):   \n",
        "  def __init__(self, input_size, hidden, drop, output_size):\n",
        "    super().__init__()\n",
        "    self.date = output_size\n",
        "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden)\n",
        "    self.drop = nn.Dropout(p = drop)\n",
        "    self.fclayers = nn.Sequential(\n",
        "        nn.Softsign(),\n",
        "        nn.Linear(in_features = hidden, out_features = output_size)\n",
        "    )\n",
        "    \n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    input_without_timestamp = inputs[:,:,1:]\n",
        "    batch_size = inputs\n",
        "    \n",
        "    _,(hidden,_) = self.lstm(input_without_timestamp)\n",
        "    output = self.drop(hidden.squeeze(0))\n",
        "    output = self.fclayers(output).squeeze(0)\n",
        "    print(output.size())\n",
        "    output = output.view(self.date,1)\n",
        "    timestamps = torch.from_numpy(np.arange(self.date) + inputs[-1,:,0].item() + 1).view(self.date,1)\n",
        "    \n",
        "    return torch.cat([timestamps,output],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr-WEhdxZilr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_twolayer(nn.Module):   \n",
        "  def __init__(self, input_size, hidden, drop, output_size):\n",
        "    super().__init__()\n",
        "    self.date = output_size\n",
        "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden, num_layers = 2, dropout = drop)\n",
        "    self.drop = nn.Dropout(p = drop)\n",
        "    self.fclayers = nn.Sequential(\n",
        "        nn.Softsign(),\n",
        "        nn.Linear(in_features = hidden, out_features = output_size)\n",
        "    )\n",
        "    \n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    _,(hidden,_) = self.lstm(inputs)\n",
        "    output = self.drop(hidden[1,:,:].squeeze(0))\n",
        "    output = self.fclayers(output).squeeze(0)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QdItl4_aEga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_sequential(nn.Module):   \n",
        "  def __init__(self, input_size, hidden, drop, num_layer=1):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layer\n",
        "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden,num_layers = self.num_layers, dropout = drop)\n",
        "    self.drop = nn.Dropout(p = drop)\n",
        "    self.fclayers = nn.Sequential(\n",
        "        nn.Softsign(),\n",
        "        nn.Linear(in_features = hidden, out_features = 1)\n",
        "    )\n",
        "    \n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    _,(hidden,_) = self.lstm(inputs)\n",
        "    output = self.drop(hidden[self.num_layers-1,:,:].squeeze(0))\n",
        "    output = self.fclayers(output).squeeze(0)\n",
        "    \n",
        "    return output\n",
        "\n",
        "#str 변환시 함수 이름이 출력되게함\n",
        "  def __str__(self):\n",
        "    return str(LSTM_sequential).split(\".\")[1][:-2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNUIr_wMZNK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(time_steps, date, data_target_batches, model, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  #prediction_list=[]\n",
        "  batch_amount = len(data_target_batches)\n",
        "  batch_size = list(data_target_batches[0][0].size())[1]\n",
        "  model.train()\n",
        "  \n",
        "  for data, target in data_target_batches:\n",
        "    \n",
        "    assert target[0,0,0].item() == (data[-1,0,0].item() + 1)\n",
        "    \n",
        "    optimizer.zero_grad()   \n",
        "    \n",
        "    data_without_timestamp = data[:,:,1:].clone()\n",
        "\n",
        "    all_predictions = []\n",
        "    \n",
        "    for d in range(date):\n",
        "      prediction_batch = model(data_without_timestamp)\n",
        "      all_predictions.append(prediction_batch)\n",
        "      data_last = data_without_timestamp[-1:,:,:].clone()\n",
        "      for b in range(batch_size):\n",
        "        data_last[0,b,1] = prediction_batch[b].item()  \n",
        "\n",
        "\n",
        "      data_without_timestamp = torch.cat((data_without_timestamp[1:,:,:],data_last),axis=0)\n",
        "\n",
        "    target_without_timestamp = torch.squeeze(target[:,:,1])\n",
        "    loss = criterion(torch.squeeze(target_without_timestamp), torch.squeeze(torch.cat(all_predictions,axis=1)))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss / batch_amount"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8SP9jUcowZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(time_steps, date, data, target, model, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  predictions=[]\n",
        "  \n",
        "  model.eval()\n",
        "  num_tries = list(target.size())[1] - (date - 1)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for i in range(num_tries):\n",
        "\n",
        "      starting_date = data[i:i+time_steps,:,:][-1,0,0].item()\n",
        "\n",
        "      input_data = data[i:i+time_steps,:,1:]\n",
        "\n",
        "      all_predictions = []\n",
        "\n",
        "      for d in range(date):\n",
        "        prediction = model(input_data)\n",
        "        all_predictions.append(prediction)\n",
        "        data_last = input_data[-1:,:,:].clone()\n",
        "        data_last[0,0,1] = prediction.item()\n",
        "\n",
        "        input_data = torch.cat((input_data[1:,:,:],data_last),axis=0)\n",
        "\n",
        "      \n",
        "      total_prediction = torch.tensor(all_predictions).to(device)\n",
        "      \n",
        "      timestamps = torch.from_numpy(np.arange(date) + starting_date + 1).to(device).view(date,1)\n",
        "      \n",
        "      prediction_with_time = torch.cat([timestamps,total_prediction.view(date,1)],axis=1)\n",
        "      \n",
        "      predictions.append(prediction_with_time.tolist())\n",
        "      loss = criterion(torch.squeeze(target)[i:i+date,1],torch.squeeze(total_prediction))\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "    \n",
        "  return epoch_loss/num_tries, np.array(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teOrriMj3_07",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8Y9vFqrs4Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def view_plot(prediction_list, target_list):\n",
        "  fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "  ax = []\n",
        "  for i in range(7):\n",
        "    ax.append(fig.add_subplot(7,1,i+1)) \n",
        "\n",
        "  step = 100\n",
        "  for i in range(1,8):\n",
        "    start = (i-1) * step\n",
        "\n",
        "    if i == 7: end = prediction_list.shape[0]\n",
        "    else: end = i * step\n",
        "\n",
        "  # try:\n",
        "    ax[i-1].plot(target_list[0:,0][start:end],target_list[0:,1][start:end])\n",
        "    ax[i-1].plot(prediction_list[:,0,0][start:end],prediction_list[:,0,1][start:end])\n",
        "    for j in range(start, end):\n",
        "      if j%foresee_date == 0:\n",
        "        ax[i-1].plot(prediction_list[:,:,0][j],prediction_list[:,:,1][j], color = 'b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDX_mXzF5lOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def view_plot_val(prediction_list, target_list):\n",
        "  fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "  ax = []\n",
        "  distribution = 2\n",
        "  for i in range(distribution):\n",
        "    ax.append(fig.add_subplot(distribution,1,i+1)) \n",
        "\n",
        "  step = 100\n",
        "  for i in range(1,distribution+1):\n",
        "    start = (i-1) * step\n",
        "\n",
        "    if i == distribution: end = prediction_list.shape[0]\n",
        "    else: end = i * step\n",
        "  # try:\n",
        "    ax[i-1].plot(target_list[0:,0][start:end],target_list[0:,1][start:end])\n",
        "    ax[i-1].plot(prediction_list[:,0,0][start:end],prediction_list[:,0,1][start:end])\n",
        "    for j in range(start, end):\n",
        "      if j%foresee_date == 0:\n",
        "        ax[i-1].plot(prediction_list[:,:,0][j],prediction_list[:,:,1][j], color = 'b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-quAwSyfsqe",
        "colab_type": "text"
      },
      "source": [
        "**hyper parameter**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k9Oi22xfqm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr = 0.0005\n",
        "# time_steps = 20\n",
        "# epoch = 50\n",
        "# batch_size = 5\n",
        "# dropout_rate = 0.3\n",
        "# num_layers = 2\n",
        "# input_size = data_np.shape[1] #open close high low volume\n",
        "# #hidden1 = 100\n",
        "# hidden = 25\n",
        "# #fc = 15\n",
        "\n",
        "# foresee_date = 5\n",
        "# shuffle = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6mP6pqzgFKZ",
        "colab_type": "text"
      },
      "source": [
        "# **create model**\n",
        "지정 디렉터리에 같은 학습모델이 있을 경우, 그 모델을 불러와서 학습\n",
        "\n",
        "1.   지정 디렉터리에 같은 학습모델이 있을 경우, 그 모델을 불러와서 학습\n",
        "2.   마운트 필수\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX_UXoojRrfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def call_model(model, num_layer, hidden_layer):\n",
        "\n",
        "  path_dir = \"/content/gdrive/My Drive/Colab Notebooks/stock_project/Models/\"\n",
        "  pretrained_model_list = os.listdir(path_dir)\n",
        "  pretrained_model_list.sort()   #가장 loss가 낮은 모델을 부르기 위해 정렬 #저장양식 model_name + \"-loss_\"+str(int(loss*1000)) ex LSTM_sequential_loss_1 #모델/loss 구분 \"-\"(하이푼)\n",
        "  i = 0\n",
        "  model_state = model.state_dict()\n",
        "  pre_train_loss = 1\n",
        "  for trained_model in pretrained_model_list:\n",
        "    i+=1\n",
        "    if str(model) == trained_model.split(\"-\")[0] and num_layer == int(trained_model.split(\"-\")[1]) and hidden_layer == int(trained_model.split(\"-\")[2]) and str(input_value)==trained_model.split(\"-\")[3]:\n",
        "      model_state = torch.load(path_dir+\"/\"+trained_model)[\"model_state\"]\n",
        "      #model.load_state_dict(torch.load(path_dir+\"/\"+trained_model)[\"model_state\"])\n",
        "      pre_train_loss = torch.load(path_dir+\"/\"+trained_model)[\"loss\"]\n",
        "      lr =torch.load(path_dir+\"/\"+trained_model)[\"lr\"]\n",
        "      optimzier = torch.load(path_dir+\"/\"+trained_model)[\"optimizer\"]\n",
        "      print(\"불러온 모델\",trained_model, \"pre_train_loss: \",pre_train_loss)   \n",
        "      break\n",
        "    if i == len(pretrained_model_list):\n",
        "      print(\"저장된 같은 모델이 없습니다\")\n",
        "  return model_state, pre_train_loss\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tg2u0H8GBsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_preprocessing(data_norm,time_steps, foresee_date):\n",
        "  slice_point = 800\n",
        "  data_train_np = data_norm[:slice_point,:]\n",
        "  data_val_np = data_norm[slice_point:,:]\n",
        "  data_train_tensor = torch.from_numpy(np.expand_dims(data_train_np, axis=1)).to(device)\n",
        "  data_val_tensor = torch.from_numpy(np.expand_dims(data_val_np, axis=1)).to(device)\n",
        "\n",
        "  target_train_np = data_train_np[time_steps:,[0,2]]\n",
        "\n",
        "  target_train_tensor = torch.from_numpy(np.expand_dims(target_train_np, axis=0)).to(device)\n",
        "  target_val_np = data_val_np[time_steps:,[0,2]]\n",
        "  target_val_tensor = torch.from_numpy(np.expand_dims(target_val_np, axis=0)).to(device)\n",
        "\n",
        "  train_batches = []\n",
        "\n",
        "  for i in range((slice_point - time_steps - (foresee_date - 1)) // batch_size):\n",
        "    data_batch = []\n",
        "    train_batch = []\n",
        "    for b in range(batch_size):\n",
        "      data_batch.append(data_train_tensor[i*batch_size + b:i*batch_size + b + time_steps,:,:])\n",
        "      train_batch.append(target_train_tensor[:,i*batch_size + b:i*batch_size + b + foresee_date])\n",
        "      \n",
        "    train_batches.append((torch.cat(data_batch,axis=1),torch.cat(train_batch,axis=0)))\n",
        "\n",
        "  return data_train_np, data_val_np, target_train_np, target_val_np, train_batches ,data_batch, data_val_tensor, data_train_tensor,target_train_tensor,target_val_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gg0e4Ho01Hic"
      },
      "source": [
        "**hyper parameter**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv08M19q1Ieo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.0005\n",
        "time_steps = 20\n",
        "epoch = 50\n",
        "batch_size = 5\n",
        "dropout_rate = 0.3\n",
        "num_layers = 2\n",
        "input_size = data_np.shape[1] #open close high low volume\n",
        "#hidden1 = 100\n",
        "hidden = 25\n",
        "#fc = 15\n",
        "\n",
        "foresee_date = 5\n",
        "shuffle = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VtBWgQU3sA2",
        "colab_type": "text"
      },
      "source": [
        "# **Model training**\n",
        "이전 모델의 loss보다 낮은 loss를 기록할 경우 자동 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3lpr12OMShP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_point = 1       ##mamximum = 5040             ################여기서 포인트 설정#########################\n",
        "count = 0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6HSYrWmlklC",
        "colab_type": "code",
        "outputId": "8c223e9f-8ea0-4f90-fc90-828675d5c25c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "source": [
        "dir = \"/content/gdrive/My Drive/Colab Notebooks/stock_project/Models/\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = data_np.shape[1]\n",
        "pre_train_loss = 1\n",
        "\n",
        "lr_list = [lr, lr/3, lr/5]\n",
        "hidden_list = list(range(10,50,5))\n",
        "num_layer_list = list(range(1,4))\n",
        "foresee_date_list = list(range(1,6))\n",
        "time_step_list = list(range(10,31))\n",
        "\n",
        "\n",
        "\n",
        "# optimizer_list = [optim.Adam(model.parameters(), lr = lr), optim.SGD(model.parameters(), lr = lr)]\n",
        "\n",
        "                     \n",
        "try:\n",
        "  for num_layer2 in num_layer_list:\n",
        "    for h_layer2 in hidden_list:\n",
        "      model = LSTM_sequential(input_size, hidden, dropout_rate, num_layer2)\n",
        "      model = model.to(device)\n",
        "      model = model.double()\n",
        "      optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "      criterion = nn.MSELoss()\n",
        "      criterion = criterion.to(device)\n",
        "      model_state, pre_train_loss =  call_model(model, num_layer2, h_layer2)\n",
        "      model.load_state_dict(model_state)\n",
        "      for time_step2 in time_step_list:\n",
        "        for foresee_date2 in foresee_date_list:\n",
        "          data_train_np, data_val_np, target_train_np, target_val_np, train_batches ,data_batch, data_val_tensor, data_train_tensor,target_train_tensor,target_val_tensor = data_preprocessing(data_norm, time_step2,foresee_date2)\n",
        "          for lr2 in lr_list:\n",
        "            count +=1\n",
        "            #print(count)\n",
        "            if count < save_point: continue         #저장지점까지 continue\n",
        "\n",
        "            # if lr < lr2 and lr != lr_list[-1]:      #만약 불러온 모델의 lr이 더 낮을경우 그 lr에 대한 학습은 건너뜀, 제일 작은 lr경우는 학습함\n",
        "            #   continue\n",
        "            print(\"lr:\",lr2, \"foresee_date:\",foresee_date2, \"h_layer:\",h_layer2, \"num_layer:\",num_layer2)\n",
        "            if shuffle:\n",
        "              random.shuffle(train_batches)\n",
        "              for ep in range(100):\n",
        "                optimizer = optim.Adam(model.parameters(), lr = lr2)\n",
        "                start_time = time.time()\n",
        "                train_loss = train(time_step2, foresee_date2, train_batches, model, optimizer, criterion)\n",
        "                end_time = time.time()\n",
        "                if ep%10 == 0:\n",
        "                  print(train_loss)\n",
        "                  print(end_time - start_time)\n",
        "                  if train_loss < pre_train_loss and train_loss < 0.003:      #이전보다 loss가 낮으면 저장함.\n",
        "                    pre_train_loss = train_loss           #제일 낮은 loss 갱신\n",
        "                    if not os.path.exists(dir):\n",
        "                      os.makedirs(dir)          #dir가 없으면 만들어서 저장\n",
        "                    save_dict = {\"model_state\":model.state_dict(),\n",
        "                                  \"loss\":train_loss,\n",
        "                                  \"lr\": lr2,\n",
        "                                  \"optimizer\": optimizer,\n",
        "                                  \"hidden_layer\": h_layer2,\n",
        "                                  \"num_layers\" : num_layer2,\n",
        "                                  \"time_step\":time_step2,\n",
        "                                  \"foresee_date\":foresee_date2,\n",
        "                                  \"criterion\":criterion,\n",
        "                                  \"input_value\":input_value}\n",
        "                    model_name = dir+str(model)+\"-\"+str(num_layer2)+'-'+str(h_layer2)+\"-\"+str(input_value)+\"-\"+'loss_'+str(int(train_loss*1000))+\".pt\"         #여러명이 돌릴경우 직관성을 위해 filename에 loss도 포함\n",
        "                    torch.save(save_dict,model_name)\n",
        "                    print(\"save\",model_name)\n",
        "                    loss_train, predictions_train = evaluate(time_step2, foresee_date2, data_train_tensor, target_train_tensor, model, criterion)\n",
        "                    view_plot(predictions_train, target_train_np)\n",
        "                    loss_val, predictions_val = evaluate(time_step2,foresee_date2, data_val_tensor, target_val_tensor, model, criterion)\n",
        "                    view_plot_val(predictions_val, target_val_np)\n",
        "except Exception as inst:\n",
        "  print(inst)\n",
        "  print(\"your check point:\", count)\n",
        "\n",
        "  "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "불러온 모델 LSTM_sequential-1-10-[1, 0, 2, 3, 4]-loss_0.pt pre_train_loss:  0.0008315561496363087\n",
            "lr: 0.0005 foresee_date: 1 h_layer: 10 num_layer: 1\n",
            "0.0009415799936748317\n",
            "0.7867929935455322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-b9be274cb0a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforesee_date2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-f5692c27bea5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(time_steps, date, data_target_batches, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtarget_without_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_without_timestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua2gSN-gufda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(url, input_value, input_size):\n",
        "  \n",
        "  model = LSTM_sequential(input_size, hidden, dropout_rate, num_layer2)\n",
        "  model = model.to(device)\n",
        "  model = model.double() \n",
        "  \n",
        "  path_dir = \"/content/gdrive/My Drive/Colab Notebooks/stock_project/Models/\"\n",
        "  pretrained_model_list = os.listdir(path_dir)\n",
        "  pretrained_model_list.sort()\n",
        "\n",
        "  i=0\n",
        "\n",
        "  for trained_model in pretrained_model_list:\n",
        "    i+=1\n",
        "    if str(input_value)==trained_model.split(\"-\")[3]:\n",
        "      state_dict = torch.load(path_dir+\"/\"+trained_model)\n",
        "      model.load_state_dict(state_dict[\"model_state\"])\n",
        "      criterion = state_dict[\"criterion\"]\n",
        "      time_steps = state_dict[\"time_step\"]\n",
        "      forsee_date = state_dict[\"foresee_date\"]\n",
        "      input_value = state_dict[\"input_value\"]\n",
        "      break\n",
        "    if i == len(pretrained_model_list):\n",
        "      print(\"저장된 같은 모델이 없습니다\")\n",
        "\n",
        "  csv_df=pd.read_csv(url)\n",
        "  csv_np_raw = csv_df.to_numpy()\n",
        "  row,col = np.shape(csv_np_raw)\n",
        "\n",
        "  full_data_np = csv_np_raw[:,1:]\n",
        "  data_np = full_data_np[:,input_value]\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(data_np)\n",
        "  data_norm = scaler.transform(data_np)\n",
        "  data_norm = np.insert(data_norm,0,np.arange(data_norm.shape[0]),axis=1)\n",
        "\n",
        "  \n",
        "  data_val_tensor = torch.from_numpy(np.expand_dims(data_norm, axis=1)).to(device)\n",
        "  \n",
        "  target_val_np = data_norm[time_steps:,[0,2]]\n",
        "  target_val_tensor = torch.from_numpy(np.expand_dims(target_val_np, axis=0)).to(device)\n",
        "  \n",
        "  loss_val, predictions_val = evaluate(time_steps,foresee_date, data_val_tensor, target_val_tensor, model, criterion)\n",
        "  \n",
        "  view_plot(predictions_val, target_val_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMoaBhJD7xug",
        "colab_type": "code",
        "outputId": "b1dff496-678f-4fa1-89d1-c297901e51c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "prediction(url, input_value,5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpqt2stv2h8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#direc = \"/content/gdrive/My Drive/Colab Notebooks/stock_project/Models/model1.py\"\n",
        "#torch.save(model.state_dict(), direc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8z5E51e4VyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model2 = LSTM_sequential(input_size, hidden, dropout_rate).to(device).double()\n",
        "#model2.load_state_dict(torch.load(\"/content/gdrive/My Drive/Colab Notebooks/stock_project/Models/model1.py\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxmCwkA-Zp4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_train, predictions_train = evaluate(time_steps, foresee_date, data_train_tensor, target_train_tensor, model, criterion)\n",
        "\n",
        "# plt.plot(target_train_np[:,1])\n",
        "# plt.plot(predictions_train[:,0,1])\n",
        "view_plot(predictions_train, target_train_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beElDM1fM916",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_train, predictions_train = evaluate(time_steps, foresee_date, data_train_tensor, target_train_tensor, model, criterion)\n",
        "\n",
        "d = 1\n",
        "#plt.figure(figsize=(100, 5))\n",
        "\n",
        "view_plot(predictions_train, target_train_np)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8IsWxsWbCf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(target_train_np[d - 1:,1])\n",
        "#print(predictions_train[:,d-1,1])\n",
        "#print(pre_with_time)\n",
        "# print(predictions_train[:,:,1][:100])\n",
        "# print(predictions_train[:,:,0][0])\n",
        "#print(predictions_train[:2,:,1])\n",
        "# print(predictions_train.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jSKakYB2N6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_train, predictions_train = evaluate(time_steps,foresee_date, data_train_tensor, target_train_tensor, model, criterion)\n",
        "\n",
        "d = 5\n",
        "view_plot(predictions_train, target_train_np)\n",
        "# plt.plot(target_train_np[d - 1:,1])\n",
        "# plt.plot(predictions_train[:,d - 1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY5lFdlI2Iaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_val, predictions_val = evaluate(time_steps,foresee_date, data_val_tensor, target_val_tensor, model, criterion)\n",
        "\n",
        "d = 1\n",
        "\n",
        "view_plot_val(predictions_val, target_val_np)\n",
        "# plt.plot(target_val_np[d - 1:,1])\n",
        "# plt.plot(predictions_val[:,d - 1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn3Queky5Yyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_val.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx0r5h1H2Pzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_val, predictions_val = evaluate(time_steps,foresee_date, data_val_tensor, target_val_tensor, model, criterion)\n",
        "\n",
        "d = 5\n",
        "view_plot_val(predictions_val, target_val_np)\n",
        "# plt.plot(target_val_np[d - 1:,1])\n",
        "# plt.plot(predictions_val[:,d - 1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5Ey9k02og27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = lr/3)\n",
        "\n",
        "for ep in range(200):\n",
        "  start_time = time.time()\n",
        "  if shuffle:\n",
        "    random.shuffle(train_batches)\n",
        "  train_loss = train(time_steps,foresee_date, train_batches, model, optimizer, criterion)\n",
        "  end_time = time.time()\n",
        "  if ep%50 == 0:\n",
        "    print(train_loss)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap9DL0vS2nE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_train, predictions_train = evaluate(time_steps,foresee_date, data_train_tensor, target_train_tensor, model, criterion)\n",
        "\n",
        "d = 1\n",
        "view_plot(predictions_train, target_train_np)\n",
        "# plt.plot(target_train_np[d - 1:,1])\n",
        "# plt.plot(predictions_train[:,d - 1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edip56o82m-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_train, predictions_train = evaluate(time_steps,foresee_date, data_train_tensor, target_train_tensor, model, criterion)\n",
        "\n",
        "d = 5\n",
        "view_plot(predictions_train, target_train_np)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1hNPHJ42m2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_val, predictions_val = evaluate(time_steps,foresee_date, data_val_tensor, target_val_tensor, model, criterion)\n",
        "\n",
        "d = 1\n",
        "view_plot_val(predictions_val, target_val_np)\n",
        "# plt.plot(target_val_np[d - 1:,1])\n",
        "# plt.plot(predictions_val[:,d - 1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-SF9ORn2mtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_val, predictions_val = evaluate(time_steps,foresee_date, data_val_tensor, target_val_tensor, model, criterion)\n",
        "\n",
        "d = 5\n",
        "view_plot_val(predictions_val, target_val_np)\n",
        "# plt.plot(target_val_np[d - 1:,1])\n",
        "# plt.plot(predictions_val[:,d - 1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNzDTD9fv0Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = lr/5)\n",
        "\n",
        "for ep in range(200):\n",
        "  start_time = time.time()\n",
        "  train_loss = train(time_steps,foresee_date, train_batches, model, optimizer, criterion)\n",
        "  end_time = time.time()\n",
        "  if ep%50 == 0:\n",
        "    print(train_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isgt8i_gWWth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_train, predictions_train = evaluate(time_steps,foresee_date, data_train_tensor, target_train_tensor, model, criterion)\n",
        "\n",
        "d = 5\n",
        "\n",
        "# print(predictions_train.shape)\n",
        "# print(target_train_np.shape)\n",
        "\n",
        "# plt.plot(target_train_np[d - 1:,1][300:500])\n",
        "# plt.plot(predictions_train[:,d - 1,1][300:500])\n",
        "\n",
        "# print(target_train_np[d - 1,0])\n",
        "# print(predictions_train[0,d-1,0])\n",
        "view_plot(predictions_train, target_train_np)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_D50oC1d2PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_val, predictions_val = evaluate(time_steps, foresee_date,data_val_tensor, target_val_tensor, model, criterion)\n",
        "\n",
        "# d = 5\n",
        "\n",
        "# plt.plot(target_val_np[d - 1:,1])\n",
        "# plt.plot(predictions_val[:,d - 1,1])\n",
        "view_plot_val(predictions_val, target_val_np)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1GRFauTG8w0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw6T2edFzKP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}